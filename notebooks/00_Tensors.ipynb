{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec50fd6",
   "metadata": {},
   "source": [
    "This notebook introduces the `Tensor` class of PyTorch, which is the fundamental class for storing numbers, whether they be data, parameters, gradients, etc. If you have any familiarity with Numpy arrays, just* treat Tensors the same.\n",
    "\n",
    "`Tensor` objects can be instantiated directly, or returned by various `torch` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350792d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21778bc4",
   "metadata": {},
   "source": [
    "## Creating a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1912c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor = Tensor([1,2,3])\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689cf40",
   "metadata": {},
   "source": [
    "Here we've stored a list of numbers in a `Tensor`.\n",
    "\n",
    "Alternatively, we could have done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a195238a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor = torch.tensor([1,2,3])\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656e86e",
   "metadata": {},
   "source": [
    "**Important points**\n",
    "- Although these are similar, `torch.tensor` has a few more options, such as `requires_grad` (more on that later)\n",
    "- If you miss out the brackets [ ], when intialising a `Tensor` from a float, they behave differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b70cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.7318e-20, 4.5848e-41]), tensor([2.]), tensor(2), tensor([2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(2), Tensor([2]), torch.tensor(2), torch.tensor([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea2a46",
   "metadata": {},
   "source": [
    "Above, `Tensor(2)` made a 2-element tensor with \"random\" floats in it, but `torch.tensor(2)` instead made a 0-dimensional tensor (basically a float) with a value of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c4dfe",
   "metadata": {},
   "source": [
    "## Tensor shapes\n",
    "\n",
    "Like Numpy arrays, Tensors can store multidimensional arrays of numbers. Each dimension can have a different number of elements. The number of dimensions that a tensor has is referred to as it's *rank*. \n",
    "E.g. a rank-1 tensor is just a vector of numbers, a rank-2 tensor is a matrix, etc.\n",
    "The *shape* of a tensor is the number of elements per dimension. This can be accessed from either the `Tensor.shape` attribute, or by calling `Tensor.size()`. The number of elements in a specific dimension can be returned by either indexing the `torch.shape`, or by passing the dimension index in the call to `size` e.g. `Tensor.size(1)`. The `len()` method is overidden to provide the size of the zeroth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28a17271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3]), 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank-1 tensor:\n",
    "r1 = Tensor([1,2,3])\n",
    "r1.shape, r1.size(), len(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2edd22fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 1]), 3, 3, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank-2 tensor:\n",
    "r2 = Tensor([[1],[2],[3]])\n",
    "r2.shape, r2.size(), r2.shape[0], r2.size(0), len(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3f275f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 2]), torch.Size([3, 1, 2]), 3, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank-3 tensor:\n",
    "r3 = Tensor([[[1,4]],[[2,5]],[[3,6]]])\n",
    "r3.shape, r3.size(), r3.shape[0], r3.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bf1353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), torch.Size([]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank-0 tensor\n",
    "r0 = torch.tensor(2)\n",
    "r0.shape, r0.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7999c",
   "metadata": {},
   "source": [
    "## Tensors from torch methods\n",
    "\n",
    "So far we've manually created tensors from lists, but `torch` has a few methods to create tensors according to specific rules by specifying the desire shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba8355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3633df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b7e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 4.6566e-10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59c50cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2803, 0.1597, 0.1032],\n",
       "        [0.3351, 0.8805, 0.5351],\n",
       "        [0.5743, 0.6463, 0.0727]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,3)  # Uniform [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "635f2e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5417, -0.3676,  0.0580],\n",
       "        [-0.9600,  0.1473,  1.4207],\n",
       "        [-0.0582,  1.1992,  0.8726]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,3)  # Normal (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85442597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "torch.ones_like(a)  # Same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "addc63ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(start=2, end=10, step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f7e35",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "Indexing tensors is the same as indexing Numpy arrays; indeces can be provided in brackets [ ], and these will return pointers to the elements at those indices in the tensor. These can be used to read and write values.\n",
    "\n",
    "When indexing a tensor, indeces can be provided per dimension, starting from the zeroth dimension. Any dimensions which are not explicitly indexed will have all their dimensions returned.\n",
    "To avoid indexing a dimension, set its index to `:`.\n",
    "Dimension indeces can be passed as:\n",
    " - A single index, e.g. `2`\n",
    " - A list of indeces, e.g. `[0,1,4,5,8]`. Indices can also be repeated and provided in any order, `[0,0,3,2]`\n",
    " - A range, e.g. `2:6` will return indices 2,3,4,5, but not 6, and `2:6:2` wil return indices 2 and 4\n",
    " - a `slice` object, e.g. `slice(6, 10, 2)` will return indices 6 and 8\n",
    " \n",
    "Indexing begins from zero, i.e. indexing a tensor at one will return the second logical element, and indexing at zero will return the first.\n",
    "\n",
    "Negative indices begin counting back from the last element, which is located at index -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8b556d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       "\n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18).reshape(3,2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98361075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 13, 14],\n",
       "        [15, 16, 17]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]  # single-element indexing of the zeroth dimension, this is the same as a[2,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10db83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4,  5],\n",
       "        [ 9, 10, 11],\n",
       "        [15, 16, 17]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,1]  # single-element indexing of the first dimension, the : returns all elements from the zeroth dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd2177c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,1]  # single-element indexing of the zeroth and first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be39977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       "\n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[ 0,  1,  2],\n",
       "         [ 3,  4,  5]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[0,1,0]]  # indexing with a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68304668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 9, 10]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [15, 16]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,:, 0:2]  # indexing with a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0304619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 9, 10]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [15, 16]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 0\n",
    "end = 2\n",
    "a[:,:,slice(start,end)]  # indexing with a slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db188f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-2]  # indexing with negative indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36e3da97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [15, 16, 17]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[0,1,2], [0,1,1]]  # Advanced indexing with two lists. Can you tell what is happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22172655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10, 10, 10],\n",
       "         [10, 10, 10]],\n",
       "\n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:1] = 10  # Setting element values\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a3600fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10, -10, -10],\n",
       "         [-10, -10, -10]],\n",
       "\n",
       "        [[  6,   7,   8],\n",
       "         [  9,  10,  11]],\n",
       "\n",
       "        [[ 12,  13,  14],\n",
       "         [ 15,  16,  17]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:1] *= -1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2381f43",
   "metadata": {},
   "source": [
    "When indexing with a single value, note that the number of dimensions is reduced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2428091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b394b",
   "metadata": {},
   "source": [
    "This can be avoided by indexing with a range of length 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c1938787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f9dab",
   "metadata": {},
   "source": [
    "## Boolean Masking\n",
    "\n",
    "Tensors can be masked to return elements for which the mask value is `True`. The mask must be of the same length as the dimension it is applied to. Masks can also be multi-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5427693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10, -10, -10],\n",
       "         [-10, -10, -10]],\n",
       "\n",
       "        [[ 12,  13,  14],\n",
       "         [ 15,  16,  17]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[True,False,True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4adaa48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10, -10, -10]],\n",
       "\n",
       "        [[  6,   7,   8]],\n",
       "\n",
       "        [[ 12,  13,  14]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,[True,False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b35c29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [ True,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6, 7, 8])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = a > 5\n",
    "print(mask)\n",
    "a[mask]  # Multi-dim mask. Note how it collapses to a rank-1 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75923c",
   "metadata": {},
   "source": [
    "Masks can also be used to write to elements that are selected by the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c3edc786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [-6, -7, -8]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "a[mask] *= -1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17868a62",
   "metadata": {},
   "source": [
    "### where function\n",
    "In some cases, it can be simpler to avoid explicitly masking tensors and instead using a `where` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2a1b414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(a < 0, a*-1, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d63efd",
   "metadata": {},
   "source": [
    "Despite the convoluted code, Multiple where statements can be chained together without having to worry about merging masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "79815acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [30, 40, 50],\n",
       "        [ 6,  7,  8]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(a < 0, a*-1, torch.where(a>2, 10*a, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9f6b8",
   "metadata": {},
   "source": [
    "### Mask functions\n",
    "`isnan` and `isinf` can be used to return Boolean masks of elements which are NaN/Inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7f5a2286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True,  True,  True,  True,  True, False,  True,  True])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.randn(10).log().isnan()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a6cc1",
   "metadata": {},
   "source": [
    "Boolean masks can be inverted with the `~` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "28c5eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False, False, False, False, False,  True, False, False])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ac3eb",
   "metadata": {},
   "source": [
    "## Data types\n",
    "\n",
    "Tensors can have various types, e.g. float, bool, int, long. They can either be intialised with the specified type, or converted between types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ce56d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6162, -1.4571, 16.2848,  2.3088,  2.8749]) tensor([-5, -1, 16,  2,  2])\n"
     ]
    }
   ],
   "source": [
    "a = 10*torch.randn(5)\n",
    "print(a, a.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc82e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True,  True,  True]) tensor([0., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "b = a > 0\n",
    "print(b, b.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6acc824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1800, 0.8382, 0.5085, 0.5282, 0.2212], dtype=torch.float64),\n",
       " tensor([0.8998, 0.1568, 0.1226, 0.9445, 0.3977]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, dtype=torch.float64), torch.rand(5, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9db80",
   "metadata": {},
   "source": [
    "## Basic mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed7cca",
   "metadata": {},
   "source": [
    "### Torch and tensor methods\n",
    "\n",
    "Like Numpy, PyTorch has many methods that can be applied `Tensor`s. These can either be called by `torch.method_name(tensor)` or `tensor.method_name()`. Generally, if it is a common mathematical operation, there is a method for it. Generally, these operations will be *broadcast* to all elements, meaning that the same operation is applied to each element, without regard for where it is located, or what the values of the other elements are. Multiple operations can be chained together. Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7d69873",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbffe82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9103,  0.9924,  0.6877],\n",
       "         [ 0.7318, -0.1450,  0.7554],\n",
       "         [ 0.9972,  0.6180,  0.9000]]),\n",
       " tensor([[ 0.4139,  0.1227, -0.7260],\n",
       "         [ 0.6815,  0.9894,  0.6552],\n",
       "         [-0.0743,  0.7862,  0.4359]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(a), a.sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d071aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1821, 0.0151, 0.6602],\n",
       "         [0.5623, 2.9458, 0.5105],\n",
       "         [0.0055, 0.8182, 0.2034]]),\n",
       " tensor([[0.6532, 0.3507,    nan],\n",
       "         [0.8659, 1.3101, 0.8453],\n",
       "         [   nan, 0.9511, 0.6716]]),\n",
       " tensor([[0.6532, 0.3507, 0.0000],\n",
       "         [0.8659, 1.3101, 0.8453],\n",
       "         [0.0000, 0.9511, 0.6716]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.square(), torch.sqrt(a), torch.sqrt(a).nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ec45e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7690e-02,  1.8618e-03, -5.3641e-01],\n",
       "        [ 4.2162e-01,  5.0561e+00,  3.6471e-01],\n",
       "        [-4.1213e-04,  7.4014e-01,  9.1744e-02]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pow(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83a15d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4267, 0.1230, 0.8125],\n",
       "        [0.7498, 1.7163, 0.7145],\n",
       "        [0.0744, 0.9046, 0.4510]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc27a098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.5322, 1.1309, 0.4437],\n",
       "         [2.1167, 5.5642, 2.0431],\n",
       "         [0.9283, 2.4708, 1.5699]]),\n",
       " tensor([[-0.3699, -0.9100,     nan],\n",
       "         [-0.1250,  0.2346, -0.1460],\n",
       "         [    nan, -0.0436, -0.3458]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.exp(), torch.log10(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35b2aa",
   "metadata": {},
   "source": [
    "### Operations between tensors and floats\n",
    "Common mathematical operators `(+,-,*,**,/,//)` are overloaded to broadcast the same operation to each element, an operations are commutative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "069f513d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8018,  0.3838, -0.4806],\n",
       "        [-1.0538, -0.8465, -0.2125],\n",
       "        [-0.8862, -1.0417,  3.2113]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d42ced5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9.1982, 10.3838,  9.5194],\n",
       "         [ 8.9462,  9.1535,  9.7875],\n",
       "         [ 9.1138,  8.9583, 13.2113]]),\n",
       " tensor([[-3.8018, -2.6162, -3.4806],\n",
       "         [-4.0538, -3.8465, -3.2125],\n",
       "         [-3.8862, -4.0417,  0.2113]]),\n",
       " tensor([[-0.0802,  0.0384, -0.0481],\n",
       "         [-0.1054, -0.0847, -0.0213],\n",
       "         [-0.0886, -0.1042,  0.3211]]),\n",
       " tensor([[-1.2472,  2.6055, -2.0808],\n",
       "         [-0.9489, -1.1813, -4.7052],\n",
       "         [-1.1285, -0.9599,  0.3114]]),\n",
       " tensor([[-0.0401,  0.0192, -0.0240],\n",
       "         [-0.0527, -0.0423, -0.0106],\n",
       "         [-0.0443, -0.0521,  0.1606]]),\n",
       " tensor([[-1.,  0., -1.],\n",
       "         [-1., -1., -1.],\n",
       "         [-1., -1.,  1.]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + 10, a-3, 0.1*a, a**-1, a/20, a//2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b0128",
   "metadata": {},
   "source": [
    "These operations can also be called as methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34553787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9.1982, 10.3838,  9.5194],\n",
       "         [ 8.9462,  9.1535,  9.7875],\n",
       "         [ 9.1138,  8.9583, 13.2113]]),\n",
       " tensor([[-3.8018, -2.6162, -3.4806],\n",
       "         [-4.0538, -3.8465, -3.2125],\n",
       "         [-3.8862, -4.0417,  0.2113]]),\n",
       " tensor([[-0.0802,  0.0384, -0.0481],\n",
       "         [-0.1054, -0.0847, -0.0213],\n",
       "         [-0.0886, -0.1042,  0.3211]]),\n",
       " tensor([[-1.2472,  2.6055, -2.0808],\n",
       "         [-0.9489, -1.1813, -4.7052],\n",
       "         [-1.1285, -0.9599,  0.3114]]),\n",
       " tensor([[-0.0401,  0.0192, -0.0240],\n",
       "         [-0.0527, -0.0423, -0.0106],\n",
       "         [-0.0443, -0.0521,  0.1606]]),\n",
       " tensor([[-1.,  0., -1.],\n",
       "         [-1., -1., -1.],\n",
       "         [-1., -1.,  1.]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(a, 10), a.sub(3), a.mul(0.1), torch.pow(a,-1), torch.div(a,20), torch.div(a,2,rounding_mode=\"floor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e41a0",
   "metadata": {},
   "source": [
    "#### In-place operations\n",
    "\n",
    "Current, these operations are performed such that they return the new value of the tensor, but the values of the original tensor are left unchanged. Many of these operations can also be performed \"in-place\", in which the original values of the tensor are updated. In torch methods, there is often an `method_` version, which performs `method` in-place on the tensor.\n",
    "Personally, I would **not** recommend, this: it is more compact, but can cause issues when building a differentiable system, since gradients can't be propagated through in-place operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f178ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1265,  0.8703, -0.1729],\n",
      "        [ 0.5711, -1.1806,  0.2918],\n",
      "        [ 1.2058, -0.4392, -0.5489]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10.1265, 10.8703,  9.8271],\n",
       "        [10.5711,  8.8194, 10.2918],\n",
       "        [11.2058,  9.5608,  9.4511]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "print(a)\n",
    "a += 10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1213e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2487,  0.2298, -0.3045],\n",
      "        [-0.3105,  0.4134,  2.0104],\n",
      "        [ 1.0289, -0.8675,  0.4059]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0213,  4.3521, -3.2839],\n",
       "        [-3.2208,  2.4192,  0.4974],\n",
       "        [ 0.9719, -1.1527,  2.4638]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "print(a)\n",
    "a.pow_(-1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "debe47dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5798, -1.1130, -1.7962],\n",
      "        [    nan, -1.2351, -0.1098],\n",
      "        [-0.5788,     nan, -1.2662]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5798, -1.1130, -1.7962],\n",
       "        [ 0.0000, -1.2351, -0.1098],\n",
       "        [-0.5788,  0.0000, -1.2662]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3).log()\n",
    "print(a)\n",
    "torch.nan_to_num_(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4effc",
   "metadata": {},
   "source": [
    "In-place operations can also happen unexpectedly, due to e.g. indexing, and can quite difficult to find and fix. Overwriting tensors with out-of-place operations is (generally) fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96bacf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a+3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd89881",
   "metadata": {},
   "source": [
    "### Operations between pairs of tensors\n",
    "Similar to floats, common mathematical operators `(+,-,*,**,/,//)` are overloaded to apply the same the same operation to each pair of elements of tensors of the same size.\n",
    "\n",
    "**Important** the `*` is an element-wise multiplication (Hadamard product). For a traditional matrix multiplication, use `torch.mm` or the `@` operator. This is subject to the normal requirements on shapes, and is not (necessarily) commutative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea879e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.4362, -0.3033, -0.8017],\n",
       "         [ 0.3703, -0.5255,  0.4169],\n",
       "         [-0.3986, -0.8594,  0.1724]]),\n",
       " tensor([[-0.0356,  0.5610, -1.1373],\n",
       "         [-0.1876,  2.2610, -1.5996],\n",
       "         [-0.9588,  1.2808,  0.7719]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = torch.randn(3,3),torch.randn(3,3)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bd7e74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.4718,  0.2577, -1.9390],\n",
       "         [ 0.1827,  1.7355, -1.1828],\n",
       "         [-1.3574,  0.4215,  0.9443]]),\n",
       " tensor([[-2.4005, -0.8643,  0.3356],\n",
       "         [ 0.5578, -2.7865,  2.0165],\n",
       "         [ 0.5603, -2.1402, -0.5995]]),\n",
       " tensor([[ 0.0868, -0.1701,  0.9118],\n",
       "         [-0.0695, -1.1882, -0.6668],\n",
       "         [ 0.3822, -1.1007,  0.1331]]),\n",
       " tensor([[   nan,    nan,    nan],\n",
       "         [1.2049,    nan, 4.0540],\n",
       "         [   nan,    nan, 0.2574]]),\n",
       " tensor([[68.3401, -0.5407,  0.7049],\n",
       "         [-1.9739, -0.2324, -0.2606],\n",
       "         [ 0.4157, -0.6709,  0.2233]]),\n",
       " tensor([[-inf, -inf,  0.],\n",
       "         [inf, -1., -1.],\n",
       "         [-inf, -1., inf]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b, a-b, a**b, a/b, a//b.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bd869fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0868, -0.1701,  0.9118],\n",
       "         [-0.0695, -1.1882, -0.6668],\n",
       "         [ 0.3822, -1.1007,  0.1331]]),\n",
       " tensor([[ 0.9124, -3.0792,  2.6370],\n",
       "         [-0.3143, -0.4466,  0.7413],\n",
       "         [ 0.0101, -1.9458,  1.9610]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b, a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d39d027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0461,  1.3932],\n",
       "        [ 0.4402,  0.4586]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,3)@torch.randn(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3bc47f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4619, -0.4401],\n",
       "        [ 1.3000,  1.2301]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.randn(2,3),torch.randn(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5d1df",
   "metadata": {},
   "source": [
    "### Implicit reshaping\n",
    "Operations between tensors of different shapes can either fail, or result in one of the tensors being automatically reshaped in order to perform the requested operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2bfc241",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.randn(2)*torch.randn(2,3) # This fails since the first tensor is of a different rank to the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e1c27edd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This fails since the first tensor is of a different rank to the second\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.randn(2,2)*torch.randn(2,3) # This fails since the first tensor has a different number of elements in the second dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7688b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6781,  0.8548,  0.0506],\n",
       "        [-0.0763,  0.4085, -1.2486]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,1)*torch.randn(2,3) # Here the first tensor is automatically copied along the second dimension to in an effective shape of (2,3) to match that of the second tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "710812b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6759,  0.3762, -0.2085],\n",
       "        [ 0.9040,  0.8564, -0.1832]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1)*torch.randn(2,3) # Here the first tensor is automatically reshaped to (1,1) and then copied along the first & second dimension to in an effective shape of (2,3) to match that of the second tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cdada",
   "metadata": {},
   "source": [
    "**Important**, this automatic reshaping can happen unexpectedly. Imagine a function that computes values based on two tensors, say the x-position and momentum of a collection of muons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "de7384e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_muons = 5\n",
    "xyz = torch.rand(n_muons,3)\n",
    "mom = torch.ones(n_muons)\n",
    "xyz.shape, mom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "71c2ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x, mom):\n",
    "    return x*mom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e138f56",
   "metadata": {},
   "source": [
    "We expect to compute one value per muon, but if we accidentally pass tensors of different shapes, the result can be reshaped unexpectedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "30be90fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = xyz[:,0:1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7d14ae89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]),\n",
       " tensor([[0.9578, 0.9578, 0.9578, 0.9578, 0.9578],\n",
       "         [0.1245, 0.1245, 0.1245, 0.1245, 0.1245],\n",
       "         [0.4745, 0.4745, 0.4745, 0.4745, 0.4745],\n",
       "         [0.6004, 0.6004, 0.6004, 0.6004, 0.6004],\n",
       "         [0.5875, 0.5875, 0.5875, 0.5875, 0.5875]]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = my_func(x, mom)\n",
    "val.shape, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "616ba87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3619, 0.9489, 0.0661, 0.2299, 0.1337])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b52773",
   "metadata": {},
   "source": [
    "Here both the `x` and `mom` tensors were shape (5,5), since the size of the `x` tensor's second dimension was 1.\n",
    "Instead, if the tensors are of the same rank, then this doesn't happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "763a50c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), tensor([0.9578, 0.1245, 0.4745, 0.6004, 0.5875]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = my_func(xyz[:,0], mom)\n",
    "val.shape, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2fbb34e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]),\n",
       " tensor([[0.9578],\n",
       "         [0.1245],\n",
       "         [0.4745],\n",
       "         [0.6004],\n",
       "         [0.5875]]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = my_func(xyz[:,0:1], mom[:,None])  # The [:,None] index adds an extra dimension to the indexed tensor\n",
    "val.shape, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88669ba4",
   "metadata": {},
   "source": [
    "## Advanced mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf1b8a",
   "metadata": {},
   "source": [
    "### Reduction methods\n",
    "Certain methods can be applied to compute values based on elements of a tensor along specified dimensions, and can reduce the number of dimensions in the tensor. Often these will take a `dim` argument for specifying the (zero-ordered) dimension along which to apply the operation. This can sometimes be a list/tuple of dimensions, to apply the operation to all dimensions listed. By default this is `None`, in which case the operation is applied to all dimensions.\n",
    "Normally, all dimensions listed, will be removed from the resulting tensor. The operations will also have a `keepdim` argument, which can be set to `True`, in order to retain the same rank; the dimension size will be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40992476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4750, 0.5390, 0.1856, 0.5409],\n",
       "         [0.4331, 0.2017, 0.5076, 0.7504]],\n",
       "\n",
       "        [[0.3849, 0.3453, 0.4623, 0.5510],\n",
       "         [0.4081, 0.5339, 0.6482, 0.0253]],\n",
       "\n",
       "        [[0.4049, 0.9674, 0.8543, 0.0608],\n",
       "         [0.5223, 0.9606, 0.7373, 0.7220]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,2,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87d74045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12.2219),\n",
       " tensor([[1.2649, 1.8516, 1.5022, 1.1527],\n",
       "         [1.3635, 1.6962, 1.8930, 1.4977]]),\n",
       " tensor([[[1.2649, 1.8516, 1.5022, 1.1527],\n",
       "          [1.3635, 1.6962, 1.8930, 1.4977]]]),\n",
       " tensor([[0.9081, 0.7406, 0.6931, 1.2913],\n",
       "         [0.7931, 0.8792, 1.1105, 0.5763],\n",
       "         [0.9272, 1.9280, 1.5916, 0.7827]]),\n",
       " tensor([[1.7405, 1.8927],\n",
       "         [1.7435, 1.6155],\n",
       "         [2.2873, 2.9423]]),\n",
       " tensor([5.7714, 6.4505]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(), torch.sum(a, dim=0), torch.sum(a, dim=0, keepdim=True), torch.sum(a, dim=1), torch.sum(a, dim=-1), a.sum([0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5acad211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5092),\n",
       " tensor([[0.2057, 0.1087, 0.0942, 0.4059],\n",
       "         [0.1571, 0.1844, 0.2997, 0.0139],\n",
       "         [0.2115, 0.9293, 0.6299, 0.0439]]),\n",
       " tensor([[0.7334, 1.1600, 0.9889, 0.7745],\n",
       "         [0.7918, 1.1174, 1.1052, 1.0416]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(), a.prod(-2), a.norm(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba68c9",
   "metadata": {},
   "source": [
    "Min, max, and median operations are a bit strange in PyTorch. In that their return types can vary, depending on whether a dimension is specified. When a dimension is specified, then the return type has both value and index attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea412884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0253)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eb0e05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.min(\n",
       " values=tensor([[0.3849, 0.3453, 0.1856, 0.0608],\n",
       "         [0.4081, 0.2017, 0.5076, 0.0253]]),\n",
       " indices=tensor([[1, 1, 0, 2],\n",
       "         [1, 0, 0, 1]])),\n",
       " tensor([[0.3849, 0.3453, 0.1856, 0.0608],\n",
       "         [0.4081, 0.2017, 0.5076, 0.0253]]),\n",
       " tensor([[1, 1, 0, 2],\n",
       "         [1, 0, 0, 1]]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = a.min(0)\n",
    "m, m.values, m.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8eff97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9674),\n",
       " torch.return_types.max(\n",
       " values=tensor([[0.4750, 0.9674, 0.8543, 0.5510],\n",
       "         [0.5223, 0.9606, 0.7373, 0.7504]]),\n",
       " indices=tensor([[0, 2, 2, 1],\n",
       "         [2, 2, 2, 0]])))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a), torch.max(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ddff8ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5076),\n",
       " torch.return_types.median(\n",
       " values=tensor([[0.4049, 0.5390, 0.4623, 0.5409],\n",
       "         [0.4331, 0.5339, 0.6482, 0.7220]]),\n",
       " indices=tensor([[2, 0, 1, 0],\n",
       "         [0, 1, 1, 2]])))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.median(a), torch.median(a, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11754579",
   "metadata": {},
   "source": [
    "## Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b4c38",
   "metadata": {},
   "source": [
    "### Reshaping tensors\n",
    "Although tensors are created with a specific shape, as we've seen they needn't retain this shape forever. Their shapes can be manipulated by the following methods, provided the number of elements remains the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1f7b8026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9]) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.Size([3, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(9)\n",
    "print(a.shape, a)\n",
    "b = a.reshape(3,3)  # reshape to the specified shape. If possible a new tensor is NOT returned, but sometimes this isn't possible, in which case the tensor is copied\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2a268",
   "metadata": {},
   "source": [
    "A similar function is `view`, but sometimes a view isn't possible due to the way that the tensor is stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "33f89d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9]) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(9)\n",
    "print(a.shape, a)\n",
    "b = a.reshape(3,-1)  # The -1 indicates that the size of this dimension should be whatever is required to retain the same number of elements\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b2495bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14],\n",
      "         [15, 16, 17]]])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 6,  7,  8],\n",
      "         [12, 13, 14]],\n",
      "\n",
      "        [[ 3,  4,  5],\n",
      "         [ 9, 10, 11],\n",
      "         [15, 16, 17]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(18).reshape(3,2,3)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "b = a.transpose(0,1)  # swaps the specifed dimensions\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c8b04841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(3,2,4)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "b = a.permute(2,0,1)  # Changes the dimensions by listing the id of the dimension in the position where it should appear in the reshaped tensor\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "560473e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(3,2,4)\n",
    "print(a.shape)\n",
    "b = a.unsqueeze(2)  # Adds an extra dimension of size 1 at position 2\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3b059cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(3,2,4)\n",
    "print(a.shape)\n",
    "b = a[:,:,None]  # A shortcut to .unsqueeze(), place None where new dimensions should be added\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f4b9b7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1, 4, 1])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(3,2,1,4,1)\n",
    "print(a.shape)\n",
    "b = a.squeeze()  # Removes all size=1 dimensions\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "442ca261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1, 4, 1])\n",
      "torch.Size([3, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(3,2,1,4,1)\n",
    "print(a.shape)\n",
    "b = a.squeeze(-1)  # Removes the size=1 dimension at the specified position\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18604b",
   "metadata": {},
   "source": [
    "#### Expanding/repeating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "63eee345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1])\n",
      "torch.Size([3, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[2, 2, 2],\n",
       "         [3, 3, 3]],\n",
       "\n",
       "        [[4, 4, 4],\n",
       "         [5, 5, 5]]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(3,2,1)\n",
    "print(a.shape)\n",
    "b = a.expand(-1,-1,3)  # \"Copies\" the tensor along singleton (size=1) dimensions the specified number of times per dimension. -1 indicates \"leave this dimension as is\"\n",
    "print(b.shape)         # This doesn't actually copy the values, instead it returns a view of the specified shape. An in place modification of one of the \"copied\"\n",
    "b                      # elements will update all elements that were \"copied\", since they share the same memory address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "30a96605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[2, 2, 2],\n",
       "         [3, 3, 3]],\n",
       "\n",
       "        [[4, 4, 4],\n",
       "         [5, 5, 5]]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0,0,0] += 1  # All elements in [0,0,:] are updated\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cafcd",
   "metadata": {},
   "source": [
    "Instead, to really copy elements, use the `repeat` and `repeat_interleave` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ca6a052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3])\n",
      "tensor([[[0, 0, 0],\n",
      "         [1, 1, 1]],\n",
      "\n",
      "        [[2, 2, 2],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [5, 5, 5]]])\n",
      "tensor([[[1, 0, 0],\n",
      "         [1, 1, 1]],\n",
      "\n",
      "        [[2, 2, 2],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [5, 5, 5]]])\n"
     ]
    }
   ],
   "source": [
    "b = a.repeat(1,1,3)  # Specify per dimension the number of times to copy data\n",
    "print(b.shape)\n",
    "print(b)\n",
    "b[0,0,0] += 1\n",
    "print(b)  # Inplace addition only affects one element; datat really was copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9330dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3])\n",
      "tensor([[[0, 0, 0],\n",
      "         [1, 1, 1]],\n",
      "\n",
      "        [[2, 2, 2],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [5, 5, 5]]])\n",
      "tensor([[[1, 0, 0],\n",
      "         [1, 1, 1]],\n",
      "\n",
      "        [[2, 2, 2],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [5, 5, 5]]])\n"
     ]
    }
   ],
   "source": [
    "b = a.repeat_interleave(3, dim=2)  # Specify the number of times to repeat along a specific dimension\n",
    "print(b.shape)\n",
    "print(b)\n",
    "b[0,0,0] += 1\n",
    "print(b)  # Inplace addition only affects one element; datat really was copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f62726",
   "metadata": {},
   "source": [
    "### Combining tensors\n",
    "There are two methods for combining multiple tensors into one:\n",
    "- cat (concatenate) combines tensors of the same along an existing dimension, in which they can potentially have different sizes\n",
    "- stack combines tensors of the same shape along a new dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c5c6594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(9).reshape(3,3)\n",
    "b = torch.arange(9,18).reshape(3,3)\n",
    "c = torch.arange(18,24).reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "213a7dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a,b,c], dim=1).shape  # combine a,b,c along the last dimension (possible even though c only has 2 elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "af7b8edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a,b], dim=0).shape  # combine a,b along the first dimension (cannot combine c since it has a different size for the remaining dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4f14f17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]), torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,b],dim=0).shape, torch.stack([a,b],dim=1).shape  # combine a,b along the new dimensions (cannot combine c since it has a different shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7950728",
   "metadata": {},
   "source": [
    "## Devices\n",
    "By default, tensors are created on the cpu, however it is often beneficial to perform operations on gpus. Tensors must either manually transfered to the gpu, or explicitly created on it.\n",
    "- Calling `.cpu()` or `.cuda()` on a `Tensor` will move it to the specified device. The `to(device)` method will place Tensors on the device.\n",
    "- Devices can be created by e.g. `torch.device('cpu')`, `torch.device('cuda')`, `torch.device('cuda:0')`.\n",
    "- Most torch function that create tensors have a `device` argument in which the user can specify that the tensor be created directly on a device, rather than shift it there from cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c8feae",
   "metadata": {},
   "source": [
    "## Retrieving data from tensors\n",
    "Whilst a lot of other Python packages can extract data from tensors, this isn't always possible. The `item()`, and `numpy()` methods can be used to export data into more portable formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6cd16fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c9d4b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6546587 , 0.9125358 , 0.566131  , 0.94550174, 0.25471312,\n",
       "       0.6733586 , 0.67005974, 0.4701813 , 0.6910274 , 0.3752662 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "103ccc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6546586751937866"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].item()  # item only exports single elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1859dbd",
   "metadata": {},
   "source": [
    "However, sometimes this isn't possible: the tensor must be on the cpu, and cannot have a gradient (more on gradient in later notebooks).\n",
    "- `.detach()` creates a view of the tensor with no gradient\n",
    "- the `.data` attribute is the values of the tensor, with no gradient\n",
    "\n",
    "The most robust way to extract data is to is to call `.detach().cpu()`  or `.data().cpu()` on the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c58f54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(1, requires_grad=True)\n",
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "517f54ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [279], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "96ae7f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94 s  29.9 ns per loop (mean  std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f7cdbe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 s  36.8 ns per loop (mean  std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad792863",
   "metadata": {},
   "source": [
    "### clone\n",
    "Sometimes it is necessary to copy a tensor. E.g. modifying the return of `a.detach()` would still affect the values of `a` and cause problems.\n",
    "Instead `a.detach().clone()` will create a new tensor with it's own memory address.\n",
    "Cloning a non-detached tensor will is differentiable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2f457b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4924, 0.1060, 0.8417, 0.5143, 0.8864, 0.1360, 0.9923, 0.0833, 0.9280,\n",
      "        0.0268])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.7084, -2.2441, -0.1724, -0.6650, -0.1206, -1.9949, -0.0078, -2.4855,\n",
       "        -0.0748, -3.6200])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10)\n",
    "print(a)\n",
    "b = torch.log_(a.detach())\n",
    "a  # a is updated by the inplace operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "dcdf8c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0172, 0.1588, 0.8579, 0.6634, 0.4513, 0.4698, 0.2617, 0.1866, 0.7338,\n",
      "        0.8747])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0172, 0.1588, 0.8579, 0.6634, 0.4513, 0.4698, 0.2617, 0.1866, 0.7338,\n",
       "        0.8747])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10)\n",
    "print(a)\n",
    "b = torch.log_(a.detach().clone())\n",
    "a  # a is not updated by the inplace operation on the clone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_tutorial_2022]",
   "language": "python",
   "name": "conda-env-pytorch_tutorial_2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
