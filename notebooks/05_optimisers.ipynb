{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f473a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280464d0",
   "metadata": {},
   "source": [
    "## Optimisers\n",
    "Optimisers are classes responsible for updating parameters according to their effect on the loss.\n",
    "The general update rule is:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\gamma\\nabla_{\\theta_t}\\!\\mathcal{L}$$\n",
    "\n",
    "The gradient on parameter $\\theta$, $\\nabla_{\\theta_t}\\!\\mathcal{L}$ can be computed by backpropagating the loss value to the parameter. The learning rate $\\gamma$ controls how much the parameter is changed by the gradient; how large a step size the optimiser makes. The fact that the change is negative, means that the loss should be such that minimising it results in better performance.\n",
    "\n",
    "The loss is often evaluated using a batch of data points, rather than all data, or just one point. The result is a trade-off between speed and precise evaluation of the loss. The stochastic nature also reduces the chance that the DNN overfits to the training data.\n",
    "\n",
    "PyTorch implements a variety of optimisers (https://pytorch.org/docs/stable/optim.html). See https://ruder.io/optimizing-gradient-descent/index.html for a good overview.\n",
    "\n",
    "We'll stick with the standard SGD for now. When instantiating an `Optimizer` the parameters to be optimised must be provided, along with the necessary hyper-parameters of the optimisation algorithm. We'll starts with a very simple set of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a5b0bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4969, -0.4201,  0.1253],\n",
       "                      [-0.4167,  0.0887,  0.2076],\n",
       "                      [-0.1430,  0.3111, -0.0112],\n",
       "                      [-0.4380,  0.1722,  0.2635],\n",
       "                      [ 0.4037,  0.2770, -0.1404],\n",
       "                      [ 0.2496, -0.2851,  0.5548],\n",
       "                      [-0.3452, -0.0351,  0.4498],\n",
       "                      [ 0.3398, -0.3619,  0.1209],\n",
       "                      [ 0.0627, -0.4354, -0.2994],\n",
       "                      [ 0.1811, -0.0945, -0.2087]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.4521,  0.1363, -0.3722, -0.1289,  0.4030,  0.3634, -0.1745,  0.3683,\n",
       "                       0.1704, -0.5234])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.2498, -0.1656, -0.1275, -0.0864,  0.2547,  0.2727, -0.0485,  0.0450,\n",
       "                       -0.0448, -0.1232]])),\n",
       "             ('2.bias', tensor([-0.1799]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(3,10), nn.ReLU(), nn.Linear(10,1), nn.Sigmoid())\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fcf64",
   "metadata": {},
   "source": [
    "To optimise the parameters of the model, we pass its `.paramaters()` generator to the optimiser constructor, which allows it to always be able to access the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e0e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(params=model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76255f",
   "metadata": {},
   "source": [
    "We also need a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab484e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b825d",
   "metadata": {},
   "source": [
    "Now we pass some data through the network to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9698f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(20,3)\n",
    "inputs[10:] += 0.25\n",
    "targets = torch.zeros(20,1)\n",
    "targets[10:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bef154de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3966],\n",
       "        [0.4726],\n",
       "        [0.5016],\n",
       "        [0.5888],\n",
       "        [0.4748],\n",
       "        [0.4353],\n",
       "        [0.5002],\n",
       "        [0.4654],\n",
       "        [0.4718],\n",
       "        [0.4801],\n",
       "        [0.4923],\n",
       "        [0.4844],\n",
       "        [0.4952],\n",
       "        [0.6057],\n",
       "        [0.5413],\n",
       "        [0.5354],\n",
       "        [0.5824],\n",
       "        [0.5801],\n",
       "        [0.4812],\n",
       "        [0.5211]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e207902",
   "metadata": {},
   "source": [
    "Now we compute the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd365fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6451, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe8242",
   "metadata": {},
   "source": [
    "At this point we want to ensure that the parameters do not have any gradient value, e.g. left over from previous updates. In this case, we can see that the `.grad` attributes are `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d61995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.4969, -0.4201,  0.1253],\n",
       "         [-0.4167,  0.0887,  0.2076],\n",
       "         [-0.1430,  0.3111, -0.0112],\n",
       "         [-0.4380,  0.1722,  0.2635],\n",
       "         [ 0.4037,  0.2770, -0.1404],\n",
       "         [ 0.2496, -0.2851,  0.5548],\n",
       "         [-0.3452, -0.0351,  0.4498],\n",
       "         [ 0.3398, -0.3619,  0.1209],\n",
       "         [ 0.0627, -0.4354, -0.2994],\n",
       "         [ 0.1811, -0.0945, -0.2087]], requires_grad=True),\n",
       " None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight, model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3240c59",
   "metadata": {},
   "source": [
    "Just in case, though we will ensure that they are all zero or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a0644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f89ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231c735",
   "metadata": {},
   "source": [
    "Now we can backpropagate the gradient of the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed23941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded84195",
   "metadata": {},
   "source": [
    "Now when we check the gradients on the parameters, we'll see that they are non-zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb6d07a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0234, -0.0042, -0.0055],\n",
       "        [ 0.0189,  0.0129, -0.0122],\n",
       "        [ 0.0031,  0.0020, -0.0027],\n",
       "        [ 0.0047,  0.0028, -0.0109],\n",
       "        [-0.0433, -0.0168, -0.0162],\n",
       "        [-0.0418, -0.0342, -0.0020],\n",
       "        [ 0.0043,  0.0025, -0.0015],\n",
       "        [-0.0071, -0.0045, -0.0005],\n",
       "        [ 0.0057,  0.0015,  0.0026],\n",
       "        [-0.0003,  0.0034,  0.0056]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd675265",
   "metadata": {},
   "source": [
    "The values of the parameters haven't changed, yet. We need to perform an update step with the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2388dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "694bf64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4967, -0.4201,  0.1254],\n",
       "        [-0.4169,  0.0886,  0.2077],\n",
       "        [-0.1431,  0.3111, -0.0112],\n",
       "        [-0.4380,  0.1722,  0.2636],\n",
       "        [ 0.4042,  0.2772, -0.1402],\n",
       "        [ 0.2500, -0.2848,  0.5548],\n",
       "        [-0.3452, -0.0352,  0.4498],\n",
       "        [ 0.3399, -0.3619,  0.1209],\n",
       "        [ 0.0626, -0.4354, -0.2995],\n",
       "        [ 0.1811, -0.0945, -0.2088]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59bbe1",
   "metadata": {},
   "source": [
    "The parameters have now updated slightly. They still have their gradients, though, which is why it is important that we always zero them before backpropagating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a63e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0234, -0.0042, -0.0055],\n",
       "        [ 0.0189,  0.0129, -0.0122],\n",
       "        [ 0.0031,  0.0020, -0.0027],\n",
       "        [ 0.0047,  0.0028, -0.0109],\n",
       "        [-0.0433, -0.0168, -0.0162],\n",
       "        [-0.0418, -0.0342, -0.0020],\n",
       "        [ 0.0043,  0.0025, -0.0015],\n",
       "        [-0.0071, -0.0045, -0.0005],\n",
       "        [ 0.0057,  0.0015,  0.0026],\n",
       "        [-0.0003,  0.0034,  0.0056]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_tutorial_2022]",
   "language": "python",
   "name": "conda-env-pytorch_tutorial_2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
