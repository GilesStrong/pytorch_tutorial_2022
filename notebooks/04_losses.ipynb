{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558e238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961c797",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "Loss functions provide a quantitative measure of the current performance of a neural network. There are many to choose from, but the most appropriate will often depend on your task and the form of the targets and outputs. Similar to layers in a neural network, losses are either offered as classes (inheriting from `nn.Module`) or as functions in `nn.functional`.\n",
    "\n",
    "PyTorch provides implementations for many common losses (https://pytorch.org/docs/stable/nn.html#loss-functions), and more advanced ones can be written by the user.\n",
    "\n",
    "In general, PyTorch losses will:\n",
    "- Take an `input` argument of predictions and a `target` argument of true values. In general, the first dimension is expected to be a batch dimension.\n",
    "- Have a *reduction* method, which determines how the final value is produced. The loss of each item in the batch will first be computed in isolation, then either these can be returned as an (N,) tensor (`reduction='none'`), or they can be reduced to the mean (`reduction='mean'` default) or the sum (`reduction='sum'`).\n",
    "\n",
    "The losses in PyTorch make strong assumptions on the inputs and targets (shapes, normalisation, log-space, logits, etc.), and often this isn't indicated in the name, so it is best check the docs to see what exactly is expected.\n",
    "\n",
    "Additionally, most losses have a `weight`, the effect of which varies between loss function and doesn't always behave as expected (to a HEP person). Additionally, they must be provided during initialisation, rather than vary per batch. If decent weight handling is required, write your own inheriting losses, or see mine: https://github.com/GilesStrong/lumin/blob/master/lumin/nn/losses/basic_weighted.py\n",
    "\n",
    "Below will be a few common losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4ecdb",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "For classification tasks with only two classes, the DNN can have a single output with a sigmoid output activation. The binary cross entropy function can then be used to quantify performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a291e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,1)  # pre-activation values of the DNN output, for a batch size of 10\n",
    "targs = torch.randint(0,2, size=(10,1)).float()  # random binary targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "497a002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34eb6cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7767)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.sigmoid(logit), targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc038b",
   "metadata": {},
   "source": [
    "This is the mean binary cross-entropy for our batch. We could instead get the raw BCE per element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f3136ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dd9740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3986],\n",
       "        [0.7126],\n",
       "        [0.5999],\n",
       "        [0.8604],\n",
       "        [1.1029],\n",
       "        [0.9895],\n",
       "        [0.8462],\n",
       "        [1.0604],\n",
       "        [0.4637],\n",
       "        [0.7328]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.sigmoid(logit), targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc1a7",
   "metadata": {},
   "source": [
    "In the above, we took the logits and applied a sigmoid activation to them, which involves taking the exponential of the logits. The BCE then compute the natural log of the predictions. One can save time and numerical precision, by instead computing the BCE directly from the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89b17d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b57bfef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7767)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba62a4",
   "metadata": {},
   "source": [
    "### Multi-label classification\n",
    "This is similar to binary classification, except now we are predicting which non-mutually-exclusive Boolean properties the inputs have. Again we can use sigmoids for each of the targets, and BCE for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3db75611",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,5)  # pre-activation values of the DNN output, for a batch size of 10 for 5 labels\n",
    "targs = torch.randint(0,2, size=(10,5)).float()  # random binary targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09933ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f17258ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7102)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.sigmoid(logit), targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e419b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9be1e7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1361, 0.5336, 0.3795, 0.5929, 0.4899],\n",
       "        [0.6156, 0.7693, 0.6194, 0.7785, 0.8370],\n",
       "        [1.1894, 0.6137, 0.7788, 0.5598, 0.7460],\n",
       "        [0.8494, 0.6064, 0.4181, 0.4173, 0.4461],\n",
       "        [0.6290, 0.8533, 1.2360, 0.4041, 0.7264],\n",
       "        [1.0350, 0.6776, 1.1268, 0.5273, 0.4180],\n",
       "        [0.4928, 0.5614, 0.4063, 0.4067, 0.7497],\n",
       "        [0.3412, 0.8677, 0.4283, 0.9006, 1.1542],\n",
       "        [0.6228, 1.1422, 0.4801, 0.4836, 1.1234],\n",
       "        [1.2205, 0.4430, 0.8429, 1.0939, 0.7363]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(torch.sigmoid(logit), targs)\n",
    "loss  # reduction none, now gives the BCE per lable per item in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8420aac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6264],\n",
       "        [0.7240],\n",
       "        [0.7775],\n",
       "        [0.5475],\n",
       "        [0.7698],\n",
       "        [0.7569],\n",
       "        [0.5234],\n",
       "        [0.7384],\n",
       "        [0.7704],\n",
       "        [0.8673]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean(-1, keepdim=True)  # we can get the mean loss per item ourselves, though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8c156",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "Extending binary classification to the case where items belong to one and only one class, and there are more than two classes. The loss here is the categorical cross-entropy, which works by comparing the predicted probabilities that an item belongs to each of the classes to the true class it belongs to. This requires that per item, the logits are normalised to one: the softmax activation will perform this normalisation. **However** none of the pyTorch CCE losses actually expect a softmaxed input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d65dc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,5)  # pre-activation values of the DNN output, for a batch size of 10 for 5 classes\n",
    "targs = torch.randint(0,5, size=(10,))  # random targets for five classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce655bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1bd5755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7859)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)  # Unlike BCELoss, the CrossEntropyLoss expects the logits. Really this should be called CrossEntropyWithLogitsLoss, but hey ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10154345",
   "metadata": {},
   "source": [
    "Alternative, if you do want to have a softmax output, there is the negative log likelihood loss, which expects... the log of the softmaxed outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba701c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af7c378a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7859)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(F.softmax(logit, dim=-1).log(), targs)  # the dim=-1 indicates to normalise over the last dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6446d",
   "metadata": {},
   "source": [
    "Alternatively, we can use the logsoftmax activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46c41274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7859)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(F.log_softmax(logit, dim=-1), targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c4a60",
   "metadata": {},
   "source": [
    "#### Multi-d multi-class classification\n",
    "If predicting the class of 2D data, or higher, the expected tensor shape for:\n",
    " - inputs is (batch, class, x, y,...)\n",
    " - targets is (batch, x, y,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae286ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,5,2,3,4)  # pre-activation values of the DNN output, for a batch size of 10 for 5 classes over a cuboid\n",
    "targs = torch.randint(0,5, size=(10,2,3,4))  # random targets for five classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85459b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cbdfbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6404)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)  # Unlike BCELoss, the CrossEntropyLoss expects the logits. Really this should be called CrossEntropyWithLogitsLoss, but hey ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38a1a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b5ab8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6404)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(F.softmax(logit, dim=1).log(), targs)  # remember to normalise over the class dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba284872",
   "metadata": {},
   "source": [
    "### Regression\n",
    "Regression problems involve predicting float targets. Typically no output activation is used, such that outputs linear map to [-inf,inf]. In such problems, the loss should scale with the error on the prediction. Common choices are:\n",
    "- squared error (p-t)**2\n",
    "- absolute error |p-t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d126bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,1)  # Outputs of the DNN output\n",
    "targs = torch.rand(10,1)  # random targets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2eeae1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()  # Mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef1a54b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1194)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "857becc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()  # L1 loss is the absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc280743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2931)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b986d6",
   "metadata": {},
   "source": [
    "## Functional losses\n",
    "As mentioned, function versions of the losses exist, too, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a409036",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,1)  # Outputs of the DNN output\n",
    "targs = torch.rand(10,1)  # random targets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cee51ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0680)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(logit, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40da17",
   "metadata": {},
   "source": [
    "## Custom loss function\n",
    "Class-based losses inherit from `nn.Module` so making our own is quite easy. We can even inherit from existing losses that are close to what we want.\n",
    "Let's make a loss that takes the squared-error on predictions and then divides it by the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7170f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalMSE(nn.MSELoss):  # Inherit from the basic MSELoss\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction='none')  # Set the reduction to none such that the se shape matches the targets\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        se = super().forward(input, target)  # Compute the MSE \n",
    "        fse = se/target\n",
    "        return torch.mean(fse)  # return the mean fractional squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5aa8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.rand(10,2)  # Outputs of the DNN output\n",
    "targs = torch.rand(10,2)  # random targets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17e01d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = FractionalMSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e161676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3215)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logit, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26111471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_tutorial_2022]",
   "language": "python",
   "name": "conda-env-pytorch_tutorial_2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
